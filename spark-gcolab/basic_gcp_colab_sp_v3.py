# -*- coding: utf-8 -*-
"""basic_gcp_colab_sp_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gEdCtrj-RpvdDw_go1A70Mv9UPfrC0jG
"""

from google.colab import auth
auth.authenticate_user()

!curl https://sdk.cloud.google.com | bash

!gcloud init

!mkdir gcc_temp

!gsutil cp gs://dhope-input-data/spark/sales.csv.txt ./gcc_temp/.

!pip install --upgrade google-cloud-bigquery-storage pyarrow

!sudo apt update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz
!pip install -q findspark
!pip install pyspark
!pip install py4j

import os
import sys
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop3.2"


import findspark
findspark.init()
findspark.find()

import pyspark

from pyspark.sql import DataFrame, SparkSession
from typing import List
import pyspark.sql.types as T
import pyspark.sql.functions as F

spark= SparkSession \
       .builder \
       .appName("Our First Spark Example") \
       .getOrCreate()

spark

from pyspark.sql.types import StructType,StructField,IntegerType,StringType,DateType

d_schema = StructType([
    StructField("product_id" ,StringType(),True),
    StructField("customer_id" ,StringType(),True),
    StructField("order_date" ,StringType(),True),
    StructField("location" ,StringType(),True),
    StructField("source_order" ,StringType(),True)
]
)

df = spark.read\
    .option("header", "false")\
    .format("csv")\
    .schema(d_schema)\
    .load("./gcc_temp/sales.csv.txt")

df.show()

import pandas as pd
import pandas_gbq

project_id = "dwhp-437913"
table_id = 'db1.tb_colab_test2'

dfp = df.toPandas()

##if_existsstr replace / fail

table_schema = [
    {'name': 'product_id', 'type': 'STRING'},
    {'name': 'customer_id', 'type': 'STRING'},
    {'name': 'order_date', 'type': 'STRING'},
    {'name': 'location', 'type': 'STRING'},
    {'name': 'source_order', 'type': 'STRING'}
]

pandas_gbq.to_gbq(dfp , table_id, project_id=project_id , table_schema=table_schema ,if_exists='append')

from google.colab import syntax

project_id = "dwhp-437913"

configuration = {
   'query': {
     "useQueryCache": True,
     "use_bqstorage_api": True,
     "parameterMode": "NAMED",
     "queryParameters": [
            {
                "name": "vlimit",
                "parameterType": {"type": "INTEGER"},
                "parameterValue": {"value": "100"},
            }]
   }
}

query = syntax.sql('''
         select * from db1.tb_colab_test2
         where cast(product_id as numeric) < @vlimit
         ''')

##df2 = pandas_gbq.read_gbq(sql, project_id=project_id)
##

df2 = pandas_gbq.read_gbq(query, project_id=project_id,configuration=configuration)

df2.count()

from google.cloud import bigquery

pid = "dwhp-437913"

client = bigquery.Client(pid)

query_str = '''
CALL `dwhp-437913.db1.sp_truncate_test2`();
'''

print (client )

query_job = client.query(query_str)
rows = list(query_job.result())
print(rows)

#get variables

q_p1 = syntax.sql('''
select parameter_value from db1.tb_parameter where process='etl_pkm'
and parameter_name = 'num_row' ''')

dfp1 = pandas_gbq.read_gbq(q_p1, project_id=project_id)
v1_p= dfp1.iat[0,0]

q_p2 = syntax.sql('''
select parameter_value from db1.tb_parameter where process='etl_pkm'
and parameter_name = 'class_pkmn' ''')

dfp2 = pandas_gbq.read_gbq(q_p2, project_id=project_id)
v2_p= dfp2.iat[0,0]

print (v1_p)
print (v2_p)

from google.cloud import bigquery

pid = "dwhp-437913"

client = bigquery.Client(pid)



query = syntax.sql('''
SELECT * FROM `dwhp-437913.db1.tb_pkmn_csv`
where cod_pok < @limit and type_pok=@state
''')

query_config = bigquery.QueryJobConfig(
    query_parameters=[
        bigquery.ScalarQueryParameter("state", "STRING", v2_p),
        bigquery.ScalarQueryParameter("limit", "INTEGER", v1_p ),
    ]
)

df = client.query(query, job_config=query_config).to_dataframe()

df

v_comand ='''
SELECT
 table_name,
 column_name,
 data_type
FROM
 dwhp-437913.db1.INFORMATION_SCHEMA.COLUMNS
WHERE
 table_name="tb_pkmn_csv2"
 '''


table_id = "db1.tb_pkmn_csv2"

job_config = bigquery.LoadJobConfig(
    schema=[
        bigquery.SchemaField("cod_pok", "INT64"),
        bigquery.SchemaField("desc_pok", "STRING"),
        bigquery.SchemaField("type_pok", "STRING"),
        bigquery.SchemaField("attack", "INT64"),
        bigquery.SchemaField("defense", "INT64"),
        bigquery.SchemaField("special_attack", "STRING")
    ]
)

job = client.load_table_from_dataframe(df, table_id, job_config=job_config)

job.result()